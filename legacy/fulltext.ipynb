{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for scraping RT\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Imports for scraping individual websites\n",
    "import requests\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Imports for data formatting\n",
    "import uuid\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODAY = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def generate_json(author, text, summary, date_created, source_url):\n",
    "    \"\"\"\n",
    "    Generate a JSON representation of a document with author and content information.\n",
    "\n",
    "    Parameters:\n",
    "    author (str): The name or identifier of the document's author.\n",
    "    text (str): The full text content of the document.\n",
    "    summary (str): A summary or brief description of the document.\n",
    "    date_created (str): The date the document was created (formatted as \"%Y %m, %d\").\n",
    "    source_url (str): The source url of the document\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary representing the document in JSON format with various attributes.\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    # get doc id\n",
    "    m = hashlib.md5()\n",
    "    m.update(text.encode('utf-8'))\n",
    "    output['documentID'] = str(uuid.UUID(m.hexdigest())) \n",
    "\n",
    "    # get authorID\n",
    "    m = hashlib.md5()\n",
    "    m.update(author.encode('utf-8'))\n",
    "    output['authorIDs'] = [str(uuid.UUID(m.hexdigest()))]\n",
    "\n",
    "    output['fullText'] = text\n",
    "    output[\"spanAttribution\"] = [{\"authorID\":output['authorIDs'][0],\n",
    "                                    \"start\":0,\n",
    "                                    \"end\":len(text)}]\n",
    "    output[\"isNeedle\"] = False\n",
    "    output[\"collectionNum\"] = \"HRS 1\"\n",
    "    output[\"source\"] = source_url\n",
    "    output[\"dateCollected\"] = TODAY\n",
    "    output[\"dateCreated\"] = date_created\n",
    "    output[\"publiclyAvailable\"] = True\n",
    "    output[\"deidentified\"] = True\n",
    "    output[\"languages\"] = [\"en\"]\n",
    "    output[\"lengthWords\"] = len(text.split(' '))\n",
    "    output[\"sourceSpecific\"] = {\n",
    "        \"authorName\": author,\n",
    "        \"rtSummary\": summary,\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDuplicates(review_json):\n",
    "    first_tokens = defaultdict(list)\n",
    "    \n",
    "    # Iterate through review_json and populate first_tokens\n",
    "    for review in review_json:\n",
    "        first_tokens[' '.join(review['fullText'].split(' ')[:5])].append(review['source'])\n",
    "    \n",
    "    # Initialize variables to keep track of the key with the most values\n",
    "    max_key = None\n",
    "    max_value_count = 0\n",
    "    \n",
    "    # Iterate through first_tokens to find the key with the most values\n",
    "    for key, value_list in first_tokens.items():\n",
    "        if len(value_list) > max_value_count:\n",
    "            max_key = key\n",
    "            max_value_count = len(value_list)\n",
    "    \n",
    "    # Print the key with the most values and its corresponding list\n",
    "    if max_key is not None:\n",
    "        print(f\"The key '{max_key}' has the most values with a count of {max_value_count}.\")\n",
    "        print(\"Corresponding list of values:\", first_tokens[max_key])\n",
    "    else:\n",
    "        print(\"No data in review_json.\")\n",
    "    \n",
    "    return first_tokens[max_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    \"\"\"\n",
    "    Scrapes and process text from a webpage, focusing on <p> tags.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned-up text extracted from the web page, or an empty string if an issue occurs during scraping.\n",
    "    \"\"\"\n",
    "    \n",
    "    # returns class attributes of an HTML element or \"NOCLASS\" if it doesn't have any.\n",
    "    def get_class(p):\n",
    "        return ''.join(p['class']) if p.has_attr('class') else \"&&NOCLASS&&\"\n",
    "\n",
    "    p_dict = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # if we've been redirected, return.\n",
    "        if response.url[-5:] != url[-5:]:\n",
    "            # print(\"redirect occured for url \" + url)\n",
    "            return \"\"\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for element in soup.find_all():\n",
    "            if element.has_attr('class') and any(\"caption\" in class_name.lower() for class_name in element['class']):\n",
    "                element.extract()\n",
    "        # finds all <p> tags\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        for p in paragraphs:\n",
    "            if len(p.text.split(' ')) > 5:\n",
    "                p_dict[get_class(p)].append(p.text)\n",
    "\n",
    "        # Find the class with the longest list of paragraphs\n",
    "        longest_key = max(p_dict, key=lambda k: len(p_dict[k]))\n",
    "\n",
    "        # Combine and clean text\n",
    "        full_text = re.sub(r'\\s+', ' ', ' '.join(p_dict[longest_key])).strip()\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(\"Exception \"  + str(e) + \" occurred for url: \" + url)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews(critic): \n",
    "    \"\"\"\n",
    "    Scrapes reviews given the name of a rotten tomatoes critic into a JSON file.\n",
    "\n",
    "    Args:\n",
    "    critic (str): The name of the critic.\n",
    "\n",
    "    Returns:\n",
    "    list: Returns a list of scraped review JSONs.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set up Chrome driver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Define and open the target URL\n",
    "    page_url = f'https://www.rottentomatoes.com/critics/{critic}/movies/'\n",
    "    driver.get(page_url)\n",
    "\n",
    "    # Initialize an empty list to save reviews\n",
    "    reviews = []\n",
    "\n",
    "    # How long to wait for next button to be clickable.\n",
    "    next_wait = WebDriverWait(driver, 3)\n",
    "    table_wait = WebDriverWait(driver, 5)\n",
    "    timeoutFlag = False\n",
    "\n",
    "    while True:\n",
    "        page_source = driver.page_source\n",
    "        # Create a Soup object and find the reviews table within it\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        table = soup.find('table', {'data-qa': 'critic-reviews-table'})        \n",
    "        if table:\n",
    "            # find all rows within the table and iterate\n",
    "            rows = table.find('tbody').find_all('tr', {'data-qa': 'row'})\n",
    "            for row in rows:\n",
    "                review_td = row.find('td', {'data-qa': 'critic-review'})\n",
    "                if review_td:\n",
    "                    date_created = datetime.strptime(review_td.find('div').find('span').text, \"%b %d, %Y\")\n",
    "                    if date_created < datetime(2021, 1, 1):\n",
    "                        review_url = review_td.find('a', string=\"Read More\")['href']\n",
    "                        if len(review_url) > 0:\n",
    "                            rt_summary = review_td.find('span').text.strip()\n",
    "                            fullText = scrape_page(review_url)\n",
    "                            if fullText != '':\n",
    "                                review = generate_json(\n",
    "                                    critic,\n",
    "                                    fullText,\n",
    "                                    rt_summary,\n",
    "                                    date_created.strftime(\"%Y-%m-%d\"),\n",
    "                                    review_url\n",
    "                                )\n",
    "                                if not timeoutFlag or review not in reviews:\n",
    "                                    reviews.append(review)\n",
    "\n",
    "                                else:\n",
    "                                    print(\"TDS timeout. Critic \" + critic + \" scraping terminated with \" + str(len(reviews)) + \" reviews.\")\n",
    "                                    driver.quit()\n",
    "                                    return reviews\n",
    "                                # print(reviews[-1])\n",
    "        try:\n",
    "            timeoutFlag = False\n",
    "            next_button = next_wait.until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'rt-button.next'))\n",
    "            )      \n",
    "            next_button.click()\n",
    "            try:\n",
    "                table_wait.until(EC.staleness_of(driver.find_element(By.CSS_SELECTOR, 'table[data-qa=\"critic-reviews-table\"]')))\n",
    "            except TimeoutException:\n",
    "                timeoutFlag = True\n",
    "                continue\n",
    "            \n",
    "        except Exception as e:  \n",
    "            break\n",
    "    \n",
    "    print(firstFiveTokens(reviews))\n",
    "    driver.quit()\n",
    "    print(\"reviews found for critic \" + critic + \": \" + str(len(reviews)))\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def get_reviews(critics):\n",
    "    review_list = []\n",
    "    for critic in critics:\n",
    "        review_list.append(scrape_reviews(critic))\n",
    "    return review_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDS timeout. Critic alonso-duralde scraping terminated with 567 reviews.\n"
     ]
    }
   ],
   "source": [
    "critics = [\"alonso-duralde\"]\n",
    "review_jsons = get_reviews(critics)\n",
    "with open(\"output/rtcorpus.jsonl\", 'a') as corpus, open(\"output/critics.txt\", 'a') as critic_list:\n",
    "    for rl in review_jsons: \n",
    "        critic = rl[0][\"sourceSpecific\"][\"authorName\"]\n",
    "        critic_list.write(critic + ': ' + str(len(rl)) + '\\n')\n",
    "        for review in rl:\n",
    "            corpus.write(json.dumps(review) + '\\n')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
