{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Imports for data formatting\n",
    "import json\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODAY = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def generate_url_json(author, urls):\n",
    "    output = {}\n",
    "    output[\"critic\"] = author\n",
    "    output[\"review_count\"] = len(urls)\n",
    "    output[\"urls\"] = urls\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews(critic):\n",
    "    \"\"\"\n",
    "    Scrapes movie review urls from a Rotten Tomatoes critic's page.\n",
    "\n",
    "    Parameters:\n",
    "    critic (str): The name or ID of the critic to scrape reviews for.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing critic information and a list of review URLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a WebDriver instance (Chrome in this case)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Construct the URL for the critic's Rotten Tomatoes page\n",
    "    page_url = f'https://www.rottentomatoes.com/critics/{critic}/movies/'\n",
    "    driver.get(page_url)\n",
    "\n",
    "    reviews = []\n",
    "\n",
    "    # wait time for the next button to become clickable\n",
    "    next_wait = WebDriverWait(driver, 3) \n",
    "    # wait time for table data to change\n",
    "    table_wait = WebDriverWait(driver, 7.5)\n",
    "\n",
    "    while True:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the review table on the page\n",
    "        table = soup.find('table', {'data-qa': 'critic-reviews-table'})\n",
    "        \n",
    "        if table:\n",
    "            # Find all rows in the table\n",
    "            rows = table.find('tbody').find_all('tr', {'data-qa': 'row'})\n",
    "\n",
    "            for row in rows:\n",
    "                review_td = row.find('td', {'data-qa': 'critic-review'})\n",
    "\n",
    "                if review_td:\n",
    "                    date_created = datetime.strptime(review_td.find('div').find('span').text, \"%b %d, %Y\")\n",
    "                    if date_created < datetime(2021, 1, 1):\n",
    "                        link = review_td.find('a', string=\"Read More\")['href']\n",
    "                        if len(link) > 0:\n",
    "                            reviews.append(link)\n",
    "        \n",
    "        try:\n",
    "            # Find and click the \"Next\" button\n",
    "            next_button = next_wait.until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'rt-button.next'))\n",
    "            )\n",
    "            next_button.click()\n",
    "\n",
    "            try:\n",
    "                # Wait for the old table to become stale\n",
    "                table_wait.until(EC.staleness_of(driver.find_element(By.CSS_SELECTOR, 'table[data-qa=\"critic-reviews-table\"]')))\n",
    "            except TimeoutException:\n",
    "                print(\"TDS timeout. Critic: \" + critic + \" scraping terminated with \" + str(len(reviews)) + \" reviews.\")\n",
    "                driver.quit()\n",
    "                return generate_url_json(critic, reviews)\n",
    "\n",
    "            time.sleep(0.15)\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "    # Quit the WebDriver instance\n",
    "    driver.quit()\n",
    "\n",
    "    # Print the number of review URLs found\n",
    "    print(\"URLs found for critic \" + critic + \": \" + str(len(reviews)))\n",
    "\n",
    "    # Return a JSON representation containing critic information and review URLs\n",
    "    return generate_url_json(critic, reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews(critic): \n",
    "    driver = webdriver.Chrome()\n",
    "    page_url = f'https://www.rottentomatoes.com/critics/{critic}/movies/'\n",
    "    driver.get(page_url)\n",
    "    reviews = []\n",
    "    next_wait = WebDriverWait(driver, 3)\n",
    "    table_wait = WebDriverWait(driver, 7.5) \n",
    "    while True:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        table = soup.find('table', {'data-qa': 'critic-reviews-table'})\n",
    "        if table:\n",
    "            rows = table.find('tbody').find_all('tr', {'data-qa': 'row'})\n",
    "\n",
    "            for row in rows:\n",
    "                review_td = row.find('td', {'data-qa': 'critic-review'})\n",
    "                \n",
    "                if review_td:\n",
    "                    date_created = datetime.strptime(review_td.find('div').find('span').text, \"%b %d, %Y\")\n",
    "                    if date_created < datetime(2021, 1, 1):\n",
    "                        link = review_td.find('a', string=\"Read More\")['href']\n",
    "                        if len(link) > 0:\n",
    "                            reviews.append(link)\n",
    "        try:\n",
    "            next_button = next_wait.until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'rt-button.next'))\n",
    "            )\n",
    "            next_button.click()\n",
    "\n",
    "            try:\n",
    "                table_wait.until(EC.staleness_of(driver.find_element(By.CSS_SELECTOR, 'table[data-qa=\"critic-reviews-table\"]')))\n",
    "            except TimeoutException:\n",
    "                print(\"TDS timeout. Critic: \" + critic + \" scraping terminated with \" + str(len(reviews)) + \" reviews.\")\n",
    "                driver.quit()\n",
    "                return generate_url_json(critic, reviews)\n",
    "\n",
    "            time.sleep(0.15)\n",
    "        except Exception as e:\n",
    "            break\n",
    "        \n",
    "    driver.quit()\n",
    "    print(\"urls found for critic \" + critic + \": \" + str(len(reviews)))\n",
    "    return generate_url_json(critic, reviews)\n",
    "\n",
    "\n",
    "def get_reviews(critics):\n",
    "    review_list = []\n",
    "    for critic in critics:\n",
    "        review_list.append(scrape_reviews(critic))\n",
    "    return review_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critics = [\"alonso-duralde\", \"david-sims\", \"jami-bernard\", \"ed-gonzalez\", \"nell-minow\", \"sara-michelle-fetters-8689\", \"valerie-complex\", \"victoria-luxford\", \"wenlei-ma\"]\n",
    "scraped = get_reviews(critics)\n",
    "with open(\"rtlinks.jsonl\", 'a') as link_jsonl, open(\"critics.txt\", 'a') as critic_list:\n",
    "    for rd in scraped: \n",
    "        critic = rd[\"critic\"]\n",
    "        critic_list.write(critic + ': ' + str(rd[\"review_count\"]) + '\\n')\n",
    "        link_jsonl.write(json.dumps(rd) + '\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
