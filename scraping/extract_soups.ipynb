{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for scraping RT\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "# Imports for data formatting\n",
    "import uuid\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODAY = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def generate_soup_json(author, text, summary, date_created, source_url):\n",
    "    \"\"\"\n",
    "    Generate a JSON representation of a document with author and content information.\n",
    "\n",
    "    Parameters:\n",
    "    document was created (formatted as \"%Y %m, %d\").\n",
    "    source_url (str): The source url of the document\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary representing the document in JSON format with various attributes.\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    # get doc id\n",
    "    m = hashlib.md5()\n",
    "    m.update(text.encode('utf-8'))\n",
    "    output['documentID'] = str(uuid.UUID(m.hexdigest())) \n",
    "\n",
    "    # get authorID\n",
    "    m = hashlib.md5()\n",
    "    m.update(author.encode('utf-8'))\n",
    "    output['authorIDs'] = [str(uuid.UUID(m.hexdigest()))]\n",
    "\n",
    "    output['fullText'] = text\n",
    "    # output[\"spanAttribution\"] = [{\"authorID\":output['authorIDs'][0],\n",
    "    #                                 \"start\":0,\n",
    "    #                                 \"end\":len(text)}]\n",
    "    # output[\"lengthWords\"] = len(text.split(' '))\n",
    "    output[\"isNeedle\"] = False\n",
    "    output[\"collectionNum\"] = \"HRS 1\"\n",
    "    output[\"source\"] = source_url\n",
    "    output[\"dateCollected\"] = TODAY\n",
    "    output[\"dateCreated\"] = date_created\n",
    "    output[\"publiclyAvailable\"] = True\n",
    "    output[\"deidentified\"] = True\n",
    "    output[\"languages\"] = [\"en\"]\n",
    "    output[\"sourceSpecific\"] = {\n",
    "        \"authorName\": author,\n",
    "        \"rtSummary\": summary,\n",
    "    }\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    try:\n",
    "        time.sleep(0.5)\n",
    "        response = requests.get(url)\n",
    "        if response.url[-5:] == url[-5:]:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            soup = str(soup).replace('\"', '\\\\\"')\n",
    "            return soup\n",
    "        return \"\"\n",
    "    except:\n",
    "        print(\"request failed for url: \" + url)\n",
    "        return \"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews(source): \n",
    "    \"\"\"\n",
    "    Scrapes reviews given the name of a source domain into a JSON file.\n",
    "\n",
    "    Args:\n",
    "    source(str): id number of the source\n",
    "e\n",
    "    Returns:\n",
    "    list: Returns a list of scraped review JSONs.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set up Chrome driver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.binary_location = \"/usr/bin/google-chrome\"\n",
    "    options.add_argument(\"--remote-debugging-port=9222\")\n",
    "    path = \"/home/mjjiang/sadiri/rt-scraping/scraping/chromedriver\"\n",
    "    driver = webdriver.Chrome(executable_path=path, options=options)\n",
    "    # Define and open the target URL\n",
    "    page_url = f'https://www.rottentomatoes.com/critics/source/{source}'\n",
    "    driver.get(page_url)\n",
    "    reviews = []\n",
    "    click_count = 0\n",
    "    review_lens = []\n",
    "    urls = set()\n",
    "    next_wait = WebDriverWait(driver, 4)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    table = soup.find('table', {'data-qa': 'critic-reviews-table'})\n",
    "    while True:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        table = soup.find('table', {'data-qa': 'critic-reviews-table'})\n",
    "        # extract reviews section from each page\n",
    "        if table:\n",
    "            # Find all rows in the table's tbody\n",
    "            rows = table.find('tbody').find_all('tr', {'data-qa': 'row'})\n",
    "            # Iterate through the rows to extract the review text\n",
    "            for row in rows:\n",
    "                # Find the review's td element\n",
    "                review_td = row.find('td', {'data-qa': 'critic-review'})\n",
    "                # find the critic's td element\n",
    "                critic_td = row.find('td',{'data-qa':'critic-review-title'}).find_next_sibling('td')\n",
    "                # Check if the review_td is found\n",
    "                if review_td:\n",
    "                    # Extract the review text\n",
    "                    date_created = datetime.strptime(review_td.find('div').find('span').text.strip(), \"%b %d, %Y\")      \n",
    "                    if(date_created < datetime(2015, 1, 1)) or \\\n",
    "                        (len(review_lens) > 5 and all(item == review_lens[-1] for item in review_lens[-5:])):\n",
    "                        driver.quit()\n",
    "                        print(\"reviews found for source \" + source + \": \" + str(len(reviews)))\n",
    "                        return reviews\n",
    "                    rt_summary = review_td.find('span').text.strip()\n",
    "                    review_url = review_td.find('a', class_=\"publication-link\")['href']\n",
    "                    if date_created < datetime(2021, 1, 1) and len(review_url) > 0 and review_url not in urls:\n",
    "                        urls.add(review_url)\n",
    "                        soup = get_soup(review_url)\n",
    "                        if soup != \"\":\n",
    "                            critic_name = critic_td.find('a')['href'].split('/')[-2]\n",
    "                            review = generate_soup_json(\n",
    "                                critic_name, \n",
    "                                soup, \n",
    "                                rt_summary, \n",
    "                                date_created.strftime(\"%Y-%m-%d\"), \n",
    "                                review_url\n",
    "                            )\n",
    "                            reviews.append(review)\n",
    "        try:\n",
    "            if(len(reviews) > 0): review_lens.append(len(reviews))\n",
    "            if(len(review_lens) % 5 == 1):\n",
    "                print(\"scraped so far: \" + str(review_lens[-1]))\n",
    "            next_button = next_wait.until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'rt-button.next'))\n",
    "            )\n",
    "            next_button.click()\n",
    "            click_count += 1\n",
    "            time.sleep(2.5)\n",
    "        except Exception as e:\n",
    "            break\n",
    "                        \n",
    "    # Close the WebDriver when done\n",
    "    driver.quit()\n",
    "    print(\"reviews found for source \" + source + \": \" + str(len(reviews)))\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def get_reviews(sources):\n",
    "    review_list = []\n",
    "    for source in sources:\n",
    "        review_list.append(scrape_reviews(source))\n",
    "    return review_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped so far: 37\n",
      "scraped so far: 236\n",
      "scraped so far: 413\n",
      "request failed for url: genre-savvy, ethically entangled military thriller by first-timer Michael Connors.\n",
      "scraped so far: 657\n",
      "reviews found for source 165: 755\n"
     ]
    }
   ],
   "source": [
    "sources = [\"165\"]\n",
    "\n",
    "review_jsons = get_reviews(sources)\n",
    "with open(\"/shared/3/projects/hiatus/rotten_tomatoes/raw_output/reviewsoups.jsonl\", 'w') as corpus:\n",
    "    for i in range(len(review_jsons)): \n",
    "        rl = review_jsons[i]\n",
    "        for review in rl:\n",
    "            corpus.write(json.dumps(review) + '\\n')\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
